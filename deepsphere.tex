\documentclass{article} % For LaTeX2e
\usepackage{iclr2019,times}
\usepackage{amssymb}
\usepackage{float}
\usepackage{hyperref}
\usepackage{url}
\usepackage{bm}
\usepackage{graphicx,array} \graphicspath{{figures/}}
\usepackage{amsmath}
\usepackage{color}
\usepackage{caption}
\usepackage{subcaption}

% % Optional math commands from https://github.com/goodfeli/dlbook_notation.
% \input{math_commands.tex}



\newtheorem{theorem}{Theorem}

\newcommand{\figref}[1]{Figure~\ref{fig:#1}}
\newcommand{\tabref}[1]{Table~\ref{tab:#1}}
\newcommand{\secref}[1]{Section~\ref{sec:#1}}
%\newcommand{\secref}[1]{\S\ref{sec:#1}}
\newcommand{\eqnref}[1]{(\ref{eqn:#1})}

\renewcommand{\b}[1]{{\bm{#1}}}   % bold symbol

% MATH SYMBOLS
\newcommand{\1}{\b{1}}              % all-ones vector
\newcommand{\0}{\b{0}}              % all-zero vector
\newcommand{\g}[1]{\b{#1}}
\newcommand{\G}{\mathcal{G}}
\newcommand{\V}{\mathcal{V}}
\newcommand{\E}{\mathcal{E}}
\newcommand{\C}{\mathcal{C}}
\newcommand{\B}{\mathcal{B}}
\renewcommand{\L}{\b{L}}
\newcommand{\tL}{\tilde{\L}}
\newcommand{\W}{\b{W}}
\newcommand{\I}{\b{I}}
\newcommand{\D}{\b{D}}
\newcommand{\U}{\b{U}}
\newcommand{\x}{\b{x}}
\newcommand{\X}{\b{X}}
\newcommand{\y}{\b{y}}
\newcommand{\Y}{\b{Y}}
\newcommand{\bu}{\b{u}}
\newcommand{\f}{\b{f}}
\newcommand{\trans}{^\intercal}
\newcommand{\R}{\mathbb{R}}
\newcommand{\bLambda}{\b{\Lambda}}
\newcommand{\blambda}{\b{\lambda}}
\newcommand{\bO}{\mathcal{O}}
\newcommand{\T}{\mathcal{T}}
\DeclareMathOperator*{\esp}{E}
\DeclareMathOperator*{\var}{Var}
\DeclareMathOperator*{\vect}{vec}
\DeclareMathOperator*{\argmin}{arg \, min}

\title{DeepSphere}
% Keywords: equivariance, spherical CNN, graph NN
%\title{DeepSphere: an equivariant spherical CNN based on a graph NN}
% Exploiting symmetries with Graph Neural Networks
% Another reason to use Graph Neural Networks: exploiting symmetries
% Another case for Graph Neural Networks: exploiting symmetries
% The case for using graphs to reason / compute / solve continuous problems.

\author{Michaël Defferrard \\
Institute of Electrical Engineering \\
EPFL, Lausanne, Switzerland \\
\texttt{michaël.defferrard@epfl.ch} \\
\And
Nathanaël Perraudin \\
Swiss Data Science Center (SDSC) \\
Zurich, Switzerland \\
\texttt{nathanael.perraudin@sdsc.ethz.ch} \\
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\newcommand{\todo}[1]{{\color[rgb]{.6,.1,.6}{#1}}}
\newcommand{\nati}[1]{{\color[rgb]{.1,.6,.1}{#1}}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}

\maketitle

\begin{abstract}
	\todo{TODO:
		\begin{enumerate}
			\item add figures from DeepSphere cosmo
			\item reorganize thoughts
			\item fill the holes
			\item coherent and consistent story
		\end{enumerate}
	}
\end{abstract}

\section{Introduction}

There has been a lot of interest recently [xx] in studying and developing NN architectures that exploit symmetries in the data by being equivariant (or invariant) to a select group of symmetric transformations (symmetry group).
In this paper, we argue that graph neural networks are a flexible model (albeit not the most general) to exploit such symmetries.
graphs are a great support for computations on many domains.

%\section{Related Work}
\todo{Related work as a paragraph. Mention interest in spherical CNNs for various applications, importance of equivariance and invariance, previous exploratory work from Renata and ourselves.}

As there is no equivalent to the uniform sampling of Euclidean space on the sphere, sphere pixelizations have been engineered to have some characteristics: equirectangular (a.k.a. equiangular or geographic) is simple\footnote{It is used in most spherical CNNs cite Cohen, renata, esteves}. HEALPix is equal-area\footnote{That is important for white noise to stay white.}, hierarchical, and has a fast SHT \citep{healpix}, GLESP features an exact (to machine precision) SHT \citep{glesp}, sympix?, cubed-sphere?.
\todo{look at the wording in sympix}
\todo{maybe have a table, or a list of desired properties from a sampling}

\todo{What is the problem we want to solve? Identify it. Our solution: allow any sampling, flexibility.}
\todo{there is a compromise about the sampling quality (how far from uniform) and how close discrete computations can be made to the true continuous ones}
masked data is a pain for ML in general (with graphs, we can just make it disappear)

\begin{figure}
	\includegraphics[height=0.3\linewidth]{example_cosmo_cmb}
	\hfill
	\includegraphics[height=0.3\linewidth]{example_ghcn_daily_tmax}
	\hfill
	\includegraphics[height=0.3\linewidth]{example_brain_meg}
	\caption{
		Examples of real data on the sphere: (left) the cosmic microwave background (CMB) temperature (K) map from Planck \citep{planck2015overview} with galactic plane masked, (middle) daily maximum temperature from the Global Historical Climatology Network (GHCN),\protect\footnotemark (right) brain activity recorded through magnetoencephalography (MEG).\protect\footnotemark
		Spherical data can be seen as a continuous function $f: S^2 \rightarrow \R^d$ that is sampled at discrete positions.
		%is sampled at convenient points, given by a chosen pixelization.
		While the sampling points can be precisely controlled in some cases (like the CCD or CMOS sensors of an omni-directional camera),
		%data is measured where the sensors are.
		% projections on the sphere, like SHREC-17 (though that's quite artificial)
		sensors might in general be non-uniformly distributed, and cover only part of the surface.
		In our examples, the position of temperature sensors cannot be globally controlled, brain activity is measured on the scalp only, and the Milky Way's galactic plane masks our observation of the CMB.
		Moreover, the sampling set might change across signals (for example, temperature sensors were added and removed throughout the survey).
		Modeling the sampled sphere as a discrete graph has the potential to faithfully and efficiently represent sampled spherical data: vertices only exist where data has been measured.
		No need to handle missing data, no need to interpolate to some predefined sampling, no waste of memory or precision due to over- or under-sampling.
		%\todo{something from climate?}
		%\todo{compare ideal full-spheres (like SHREC-17 projection from Esteves, 360 from Coors) to real-world measurements?}
		%\todo{colorbar for brain}
	}
	\label{fig:spherical_data_examples}
\end{figure}
\footnotetext{\url{https://www.ncdc.noaa.gov/ghcn-daily-description}}
\footnotetext{Example data from MNE, \url{https://martinos.org/mne/stable/auto_tutorials/plot_visualize_evoked.html}.}

\todo{Most of our view is shaped by cosmo. What is going on in other fields (climate, weather)? Do they have their own samplings? Cosmo seems the most advanced for SP on the sphere.}

\todo{We can take here a different stance than: "we'll use whatever method gives the best performance". SHTs are the standard analysis tool on the sphere. People not only want to apply learned filters, but designed ones (e.g., Gaussian smoothing), people want to interpret and see the spectrum as spectral analysis is important (at least in cosmo, we should find some refs for that}

\todo{From \citet{kondor2018equivariance}:
* convolution implies equivariance to the action of some group
* to be equivariant (to exploit symmetries in the data), you need convolution
* often easier to describe in Fourier space => motivation for the spectral interpretation?
}

\subsection{The sphere}

$S^2 = SO(3) / SO(2) (/ = modulo)$

$S^2$ is an homogeneous space of $SO(3)$.

\section{Method}

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{figure_architecture_v3}
	\caption{Overall NN architecture, showing here three convolutional layers acting as feature extractors followed by three fully connected layers with softmax acting as the classifier.
    A convolutional layer is based on five operations: convolution, non-linearity, batch normalization, down-sampling, and pooling.
	While most operations are agnostic to the data domain, the convolution and the down-sampling have to be adapted.
	In this paper, we propose first to model the sphere with a graph and to perform the convolution on the graph.
	Graphs are versatile data structures which can model any sampling, even irregular or partial.
	Second, we propose to exploit a hierarchical pixelization of the sphere for the down-sampling operation.
	It allows the NN to analyze the data at multiple scales while preserving the spatial localization of features.
	This figure shows a network that operates on the whole sphere.
	The process is the same when working with partial observations, except that the graph is only built for the region of interest.}
	\label{fig:architecture}
\end{figure}


In this work, we use the graph neural network proposed by \citet{defferrard2016gnn}.
While there exists many graph NNs (see for example xxx and xxx for recent reviews),
\todo{Why this graph NN? We want an equivalence with the continuous world to prove equivariance to arbitrary rotations.}


\todo{Basics of GSP, how the graph is built, polynomial filters.}
% That should be short: it's an ML audience. We can reference previous work. No general ML stuff, as in cosmo paper.

%\section{Spherical harmonics and equivariance}
\section{Harmonics and equivariance}

% \todo{why equivalence to continuous provides equivariance. Clean way of doing it. Alternative: Renata's mechanical approach.}

\paragraph{Why is the harmonic analysis of the graph important for equivariance?} 
As both the graph and the spherical convolution can be expressed as a multiplication in the Fourier domain, the spectral analysis of the graph Laplacian provides valuable insights on the equivariance property of the graph convolution. 
The rationale being that as the spectral bases align, the two operations become more similar, with the consequence that the equivariance property of the spherical convolution being carried out to the graph convolution. 


\paragraph{Example:} 
In order to better understand this process, let us study the circle Manifold $\mathcal{C}^1$ as depicted in \figref{ring}. We parametrize each point $(\cos\theta,\sin\theta)$ of $\mathcal{C}^1$ with the variable $\theta\in[0,2\pi[$.
For this manifold, the eigenfunctions of the Laplace-Beltrami operator can be expressed as $u_\ell(\theta)=e^{i \theta m \ell}$, for $\ell \in \mathbb{N}$ and $m\in\{-1,1\}$. 
Interestingly, if one selects a \emph{regular sampling} on the $\mathcal{C}^1$, as shown in \figref{ring}, the sampled eigenfunctions turn out to be the discrete Fourier basis, meaning that the harmonic decomposition of a discretized function on the circle can be done using the well-known Discrete Fourier Transform (DFT). 
In this case, it turns out that the DFT is also an eigenbase of the of $2$ nearest neighbors graph Laplacian \cite{strang1999discrete}. Hence, in the $\mathcal{C}^1$ case, it can be verified, under mild assumptions, that the graph convolution is equivariant to translation\footnote{See \cite[section 2.2 and Equation3]{perraudin2017stationary} for details.}  
It should be noted that if an irregular sampling is selected, then the graph Fourier basis will not align the sampled manifold eigenvectors. 

The argument for the graph Laplacian to be diabonizable with the DFT basis is simple: a) the graph Laplacian is symmetric circulant and that all circulant matrices have complex exponentials as eigenbases. 
More generally, higher dimension circles, such as the torus $\mathcal{C}^2$, have also a circulant weight matrix and hence their associated graph is also equipped with an equivariant convolution.

\paragraph{Necessary conditions for the rotational equivariance on the sphere}
We note two important points for the equivariance property to be carried on the graph convolution: a) the sampling needs to be regular, and b), the constructed graph has to be constructed circulant,\footnote{A circulant graph is an undirected graph acted on by a cyclic group of symmetries which takes any vertex to any other vertex.} i.e. it has a circulant weight matrix.
Unfortunately, there exist no regular sampling for a sphere $\mathcal{S}^2$ of more than $12$ points. Hence it is in general not possible to build a graph with a convolution that is equivariant to rotation\footnote{The exception is the dense graph. Unfortunately, in the extreme case the graph convolution is limited to Kronkeker kernels, which makes it useless.}. 



% \subsection{Boundary conditions}

% The graph setting used throughout this contribution corresponds to assumed reflective border conditions.
% While that is irrelevant when working on the complete sphere (as it has no border), it slightly affects the convolution operation when only a part of the sphere is considered.
% As depicted in \figref{border_effects}, a filter localized near a border (via $h(\L) \b \delta_i$) is no longer isotropic.
% These border effects can, however, be mitigated by padding with zeros a small area around the part of interest (in which case they become similar to border effects in classical CNNs).
% We however do not expect these effects to cause any problem as long as the data samples cover the same area.

% \begin{figure}[ht!]
% 	\centering
% 	\includegraphics[width=\linewidth]{border_effects}
% 	\caption{Convolution kernel (also called filter) localized in the center and left corner of a graph built from $1/12^\text{th}$ of the sphere at $N_{side} = 16$.
% 		A filter $h$ is localized on a pixel $i$ as $\T_i h = h(\L) \b \delta_i$ (see \secref{graph_convolution}, equation~\eqnref{graph_convolution_spatial}).
% 	The filter is not isotropic anymore when localized on the corner as the graph representation of a manifold assumes reflective border conditions.}
% 	\label{fig:border_effects}
% \end{figure}


% Proper study of symmetries and equivariance requires a continuous treatment.
% @Michael: Not necessarily. It is more complicated than that. For example, if you work in the spectral domain, you can bypass sampling problem. This is what happen with the work of Cohen. So I think we should not speak about that. 
% For example, only rotations by $90^\circ$ can be studied on a discrete sampling grid.


% \subsection{Convergence proof for random unform sampling}

% Let us use the angular parametrization of the sphere $\mathcal{S}^2=\{\bf{x} | \|\bf{x}\|_2=1 \}$, in term of angles of zenith, $\theta\in [0,\pi ]$, and azimuth $\phi\in [0,2\pi[$.
% \begin{equation}
% \Phi(\theta, \phi) = (\sin \theta \cos \phi, \sin \theta \sin \phi, \cos \theta )
% \end{equation}
% Let $\mathcal(C)^\infty(\mathcal{S}^2)$ denote the class of infinitely differentiable functions $\mathcal{S}^2\rightarrow \mathbb{R}$. Using the agnular parametrization of the sphere, the Laplace-Beltrami for a function $f\in \mathcal(C)^\infty(\mathcal{S}^2)$ is given by:
% \begin{equation}
% \Delta_{\mathcal{S}^2} f = \frac{1}{\sin \theta } \frac{\partial}{\partial \theta} \left(\sin \theta  \frac{\partial f}{\partial \theta} + \frac{1}{\sin^2 \theta} \frac{\partial^2f}{\partial \phi^2} \right)
% \end{equation}

% Let us extend the graph Laplacian 

% \begin{equation}
% \L \b{f}[i] := \frac{1}{n} \left( \b{f}[i]\sum_{j=1}^n k_t(x_i,x_j) -\sum_{j=1}^n \b{f}[j] k_t(x_i,x_j)\right),
% \end{equation}
% where $k_t(x_i,x_j)=e^{-\frac{x_i-x_j}{4t}}$, \label{eq:full_graph_gaussian_weight}
% to each point on the Sphere.
 
%  It is a generalization of the graph Laplacian operator that apply on a function $f:\mathcal{S}^2 \rightarrow \mathbb{R} $:
% \begin{equation}
% L_t f(y) := \frac{1}{n} \left( f(y)\sum_{j=1}^n k_t(y,x_j)-\sum_{j=1}^n f(x_j)k_t(y,x_j) \right)
% \end{equation}


%  Given a function $f\in \mathcal{C}^\infty(\mathcal{S}^2)$ and for a fixed point $p \in \mathcal{C}^2$, after appropriate scaling the operator $L_t$, converge toward of the  Laplace–Beltrami operator.

% \begin{theorem}[Adaptation of Theorem 3.1 of~\cite{belkin2005towards}, Laplacian convergence] \label{theo:laplacian_convergence_belkin}
% Let data points $\b{x}_1,\dots, \b{x}_N$ be sampled from a uniform distribution on a manifold $\mathcal{M} \subset \mathbb{R}^T$. 
% Using the weight function \eqref{eq:full_graph_gaussian_weight}, define a sequence $t_N=N^{-\frac{1}{4+\alpha}}$, where $\alpha>0$ and let $f\in \mathcal{C}^\infty(\mathcal{C}^2)$, then the following holds:
% \begin{equation}
% \lim_{N \rightarrow \infty} \frac{1}{t_N(4\pi t_N)} (L \b{f})[i] = \frac{1}{\text{vol} (\mathcal{C}^2)} (\Delta_{\mathcal{C}^2} f)(\b{x}_i)
% \end{equation}
% where the limit is taken in probability and $\text{vol}(\mathcal{C}^2)=4 \pi$ is the surface of the sphere with respect to the canonical measure.
% \end{theorem}

% \subsection{Empirical analysis of the graph eigenvectors}

% The first 16 eigenvectors $[\b u_1, \ldots, \b u_{16}]$ of the graph Laplacian $\L$, forming the lower part of the graph Fourier basis $\U$, are shown in \figref{graph_harmonics}.

\paragraph{Empirical analysis of DeepSphere graph Laplacian:} 
% Let us further observe the spectral properties of our constructed spherical graph laplacian $\L$.
Given the graph used for DeepSphere, let us analyse the spectral alignment of the graph Fourier basis and the spherical harmonics.
The graph Laplacian eigenvalues, shown in \figref{graph_eigenvalues}, are clearly organized in frequency groups of $2\ell + 1$ orders for each degree $\ell$.
% We remind the reader that the amplitude of the Laplacian eigenvalue is proportional to the sum of the variations of its associated eigenvector.
% Furthermore, all spherical harmonics with the same order $\ell$ have the same variation. 
% Hence, the fact that the Laplacian eigenvalues are grouped in blocks of size $2\ell + 1$ is a hint that the graph eigenvectors approximate the spherical harmonics.
These blocks corresponds to the different eigenspaces of the spherical harmonics.
We also show the correspondence between the subspaces spanned by the graph Fourier modes and the spherical harmonics in \figref{subspace_harmonics_eigenvectors}.
%The block-diagonal structure of the matrix, as opposed to diagonal, results the fact that the phase of the graph eigenvector is not aligned with the one of the spherical harmonics.
For $N_{side}=4$, we clearly observe a good allignment for $\ell\leq 8$. This mean that for kernel with low frequencies, the graph convolution will be rotational equivariant. On the contrary, a kernel with higher frequencies ($\ell>8$) will not be.

% While these indications show that the constructed graph Fourier basis approximates well the spherical harmonics, 
% one should not forget that 
The imperfection in the allignment is likely due to the small irregularities in the HEALPix sampling (non-constant number of neighboring pixels and varying distance between pixels, \todo{see \figref{healpix_graph_4})} have an important effect on the graph Fourier modes.
% First, we believe that they are responsible for energy leaking across frequency bands in \figref{subspace_harmonics_eigenvectors}.
Furthermore, the graph construction scheme might not be optimal as the resulting Laplacian is probably far from circulant. One construction scheme that might be more adapted is a fully connected graph as this scheme is used in convergence proof in \cite{belkin2005towards,belkin2007convergence}.
% Second, as the resolution is increased with $N_{side} \rightarrow \infty$ and $N_{pix} \rightarrow \infty$, we are still unsure if the eigenvectors would converge towards the spherical harmonics.
% We believe that it might be the case for a different construction of the graph (fully connected) and that a proof could be built on top of the work of~\cite{belkin2007convergence}. That is, however, out of the scope of this contribution.
% The theoretical study of those phenomenons is left as future work.
% Third, counter-intuitively, some eigenvectors will be localized~\citep{perraudin2018global}, i.e., they will span a small part of the sphere.
% Those discrepancies result in a convolution operation that is not exactly equivariant to rotation.
% Nevertheless, our experiments suggest that these downsides do not have an important effect on the convolution nor hinder the performance of the NN.

\begin{figure}[ht!]
\centering
\begin{minipage}{.2\textwidth}
	\centering
	\includegraphics[width=\linewidth]{ring}
	\captionof{figure}{Green: Circle Manifold $\mathcal{C}^1$. Red: the regular sampled points and graph vertices. Blue: graph edges.}
	\label{fig:ring}
\end{minipage}
\hspace{1cm}
\begin{minipage}{.7\textwidth}
	\centering
	\includegraphics[width=\linewidth]{graph_eigenvalues}
	\captionof{figure}{The eigenvalues $\bLambda$ of the graph Laplacian $\L = \U \bLambda \U\trans$, which corresponds to squared frequencies, are clearly organized in groups. Each group corresponds to a degree $\ell$ of the spherical harmonics. Each degree has $2\ell + 1$ orders. 
	See also \figref{graph_harmonics}.
	}
	\label{fig:graph_eigenvalues}
\end{minipage}
\end{figure}

\begin{figure}[ht!]
	\centering
	\includegraphics[width=0.9\linewidth]{subspace_harmonics_eigenvectors_v2}
	\caption{Correspondence between the subspaces spanned by the graph Fourier modes and the spherical harmonics.
		The correspondance is computed in two steps. (i) We compute the PSD of each graph eigenvector.
		(ii) As, for each harmonic index $\ell$, there is $2\ell+1$ spherical harmonics, we sum the contribution of the corresponding $2\ell+1$ graph eigenvectors.
		% No real convergence with increasing Nside.
		% Row $j$ is the sum of the (normalized) power spectral densities (PSDs), computed by the SHT, of the $2j+1$ graph Fourier modes $\b u_i$ identified with degree $j$.
		% (\figref{graph_eigenvalues} shows how Fourier modes can be identified to spherical degrees.)
		% The PSD is computed for degrees $\ell=0$ to $\ell = 3 N_{side} -1$.
		% Pixel $k$ in row $j$ measures the energy in frequency band $k$ of the graph Fourier modes identified to be part of band $j$.
		The matrix should ideally be the identity.
		While not the case, the plot shows how the subspaces agree: the subspaces spanned by the Fourier modes correspond to those spanned by the spherical harmonics in the low frequencies, while the Fourier modes leak towards adjacent frequency bands in the high frequencies.
		While there is a systematic error, the Fourier modes agree at higher frequencies as $N_{side}$ increases.}
		\label{fig:subspace_harmonics_eigenvectors}
\end{figure}



\todo{new results from Martino: becomes better in BN setting, currently under study}

\todo{we have less rich operations (compared to the most general linear equivariant map) by restricting our filters to be radial, but does it matter in practice?}
\todo{isotropic filters provide invariance to the third rotation}

\section{Experiments}

\todo{the cosmo experiment}

\todo{maybe mention some preliminary results on SHREC-17: we could say performance are so far mostly similar (while being invariant to the third rotation), still under study}

\section{Conclusion}  % & perspective

\todo{Generalization vs specialization: most general is to assume no symmetries (fully connected NN).
NNs can be specialized by adding some equivariance and invariance to symmetry groups such as translation, rotation, flip, etc.
More specialized NNs are more limited in the class of functions they can approximate, but requires less samples.
Again, use the right symmetries.}

\todo{As equivariance is not the Graal either. We expect the desired amount of equivariance to depend on the data and task. As usual, practitioners should adopt a NN architecture that exploit the symmetries of their problem. Cite paper with difference operators that is not equivariant but performs better than Cohen.}

\todo{future directions: equivalence / convergence to spherical harmonics, other tasks, comparison with other spherical CNNs, graph CNNs (especially those for manifolds), boundary conditions on partial sphere}

\textbf{Long term vision.}

We hope to establish graphs as a generic support for processing and learning over known and unknown manifolds.
\todo{Applications are plenty:
known manifolds (1D, 2D, 3D Euclidean spaces, circle, sphere)
% Minkowski spacetime
unknown manifolds (shapes, point clouds, feature sets)
}
\todo{
* known manifolds: grid, sphere, spacetime (a pseudo-Riemannian 4-manifold), surfaces defined by NURBS, anything else?
* unknown manifolds: meshes (human body), point clouds => what are the symmetries? local isometry
* non-manifold: all networks (brain, social, transportation, telecom, etc.) and relations (author-papers, user-products)
}

\todo{Similar goal: graph CNNs for group equivariant convolution.}

The vision is to push the use of graphs as the support for computation on manifolds. The advantages of graphs are to relax constraints on the sampling (e.g., allow an irregular or partial sampling), and be computationally more efficient (while being exact w.r.t. the continuous case).
Part of a broader research effort to explore the use of GSP to solve continuous problems.

\subsubsection*{Acknowledgments}

\todo{Pierre, Cardoso, Tomek?}

\bibliography{refs}
\bibliographystyle{iclr2019}

\end{document}
