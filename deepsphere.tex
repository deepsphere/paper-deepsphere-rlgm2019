\documentclass{article} % For LaTeX2e
\usepackage{iclr2019,times}
\usepackage{amssymb}
\usepackage{float}
\usepackage{hyperref}
\usepackage{url}
\usepackage{bm}
\usepackage{graphicx,array} \graphicspath{{figures/}}
\usepackage{amsmath}
\usepackage{color}
\usepackage{caption}
\usepackage{subcaption}

% % Optional math commands from https://github.com/goodfeli/dlbook_notation.
% \input{math_commands.tex}

\newtheorem{theorem}{Theorem}

\newcommand{\figref}[1]{figure~\ref{fig:#1}}
\newcommand{\Figref}[1]{Figure~\ref{fig:#1}}
\newcommand{\tabref}[1]{Table~\ref{tab:#1}}
\newcommand{\Tabref}[1]{table~\ref{tab:#1}}
\newcommand{\secref}[1]{Section~\ref{sec:#1}}
\newcommand{\Secref}[1]{section~\ref{sec:#1}}
%\newcommand{\secref}[1]{\S\ref{sec:#1}}
\newcommand{\eqnref}[1]{(\ref{eqn:#1})}

\renewcommand{\b}[1]{{\bm{#1}}}   % bold symbol

% MATH SYMBOLS
\newcommand{\1}{\b{1}}              % all-ones vector
\newcommand{\0}{\b{0}}              % all-zero vector
\newcommand{\g}[1]{\b{#1}}
\newcommand{\G}{\mathcal{G}}
\newcommand{\V}{\mathcal{V}}
\newcommand{\E}{\mathcal{E}}
\newcommand{\C}{\mathcal{C}}
\newcommand{\B}{\mathcal{B}}
%\newcommand{\circle}{\mathcal{C}^1}
\newcommand{\torus}{\mathcal{C}^2}
\newcommand{\sphere}{\mathcal{S}^2}
\renewcommand{\L}{\b{L}}
\newcommand{\tL}{\tilde{\L}}
\newcommand{\W}{\b{W}}
\newcommand{\I}{\b{I}}
\newcommand{\D}{\b{D}}
\newcommand{\U}{\b{U}}
\newcommand{\x}{\b{x}}
\newcommand{\X}{\b{X}}
\newcommand{\y}{\b{y}}
\newcommand{\Y}{\b{Y}}
\newcommand{\bu}{\b{u}}
\newcommand{\f}{\b{f}}
\newcommand{\trans}{^\intercal}
\newcommand{\R}{\mathbb{R}}
\newcommand{\bLambda}{\b{\Lambda}}
\newcommand{\blambda}{\b{\lambda}}
\newcommand{\bO}{\mathcal{O}}
\newcommand{\T}{\mathcal{T}}
\DeclareMathOperator*{\esp}{E}
\DeclareMathOperator*{\var}{Var}
\DeclareMathOperator*{\vect}{vec}
\DeclareMathOperator*{\argmin}{arg \, min}

\title{DeepSphere}
% Keywords: equivariance, spherical CNN, graph NN
%\title{DeepSphere: an equivariant spherical CNN based on a graph NN}
%\title{DeepSphere: a graph-based equivariant spherical CNN}
% Exploiting symmetries with Graph Neural Networks
% Another reason to use Graph Neural Networks: exploiting symmetries
% Another case for Graph Neural Networks: exploiting symmetries
% The case for using graphs to reason / compute / solve continuous problems.

\author{Michaël Defferrard \\
Institute of Electrical Engineering \\
EPFL, Lausanne, Switzerland \\
\texttt{michaël.defferrard@epfl.ch} \\
\And
Nathanaël Perraudin \\
Swiss Data Science Center (SDSC) \\
Zurich, Switzerland \\
\texttt{nathanael.perraudin@sdsc.ethz.ch} \\
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\newcommand{\todo}[1]{{\color[rgb]{.6,.1,.6}{#1}}}
\newcommand{\nati}[1]{{\color[rgb]{.1,.6,.1}{#1}}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}

\maketitle

\begin{abstract}
	Spherical data is found in many applications.
	By modeling the discretized sphere as a graph, we can accommodate non-uniformly distributed, partial, and changing samplings.
	Moreover, graph convolutions are computationally more efficient than spherical convolutions.
	As equivariance is desired to exploit rotational symmetries, we present preliminary results on the rotation equivariance of the graph neural network introduced in \citet{defferrard2016convolutional}.
	Experiments show good performance on rotation-invariant learning problems.
\end{abstract}

\begin{figure}[h]
	\includegraphics[height=0.3\linewidth]{example_cosmo_cmb}
	\hfill
	\includegraphics[height=0.3\linewidth]{example_ghcn_daily_tmax}
	\hfill
	\includegraphics[height=0.3\linewidth]{example_brain_meg}
	\caption{
		Examples of real data on the sphere: (left) the cosmic microwave background (CMB) temperature from \citet{planck2015overview},
		%with galactic plane masked
		(middle) daily maximum temperature from the Global Historical Climatology Network (GHCN),\protect\footnotemark (right) brain activity recorded through magnetoencephalography (MEG).\protect\footnotemark
		For those examples, a rigid full-sphere pixelization is not ideal: the Milky Way's galactic plane masks observations, brain activity is measured on the scalp only, and the position and density of temperature sensors is arbitrary and changes over time.
		%\todo{something from climate, weather?}
		%\todo{compare ideal full-spheres (like SHREC-17 projection from Esteves, 360 from Coors) to real-world measurements?}
		%\todo{horizontal colorbar for brain}
	}
	\label{fig:examples}
\end{figure}
\footnotetext{\scriptsize\url{https://www.ncdc.noaa.gov/ghcn-daily-description}}
\footnotetext{\scriptsize\url{https://martinos.org/mne/stable/auto_tutorials/plot_visualize_evoked.html}}

\section{Introduction}

% \todo{sphere only, or start more general?}
% 1. general
% 2. sphere
% 3. equivariance to symmetries (we might not necessary want that)

% \todo{What is the problem we want to solve? Identify it. Our solution: flexible (any sampling) and computational efficient.}

% \todo{Related work: mention interest in spherical CNNs for various applications, importance of equivariance and invariance, previous exploratory work with graphs from Renata and ourselves.}

Graphs have long been used as models for discretized manifolds: for example to smooth meshes \citep{taubin1996meshsmoothing}, reduce dimensionality \citep{belkin2003laplacian}, and, more recently, to process signals \citep{shuman2013gsp}.
In most works, it is assumed that the true data manifold is unknown to the processing or learning method, and is only observed through discrete samples.
In this contribution, we want to make a case for using graphs as a support of computation on known manifolds.\footnote{For example, the plane where images live is known, while the true surface of a point cloud is unknown.}

%From now on, we focus on the sphere.
Along
%$d$-dimensional
Euclidean spaces, the sphere is one of the most commonly encountered manifold: it is notably used to represent omnidirectional images, global weather and climate data, cosmological observations, and brain activity measured on the scalp (see \figref{examples}).
Spherical convolutional neural networks (CNNs) have been developed to work with most of those modalities \citep{cohen2018sphericalcnn, esteves2017sphericalcnn, perraudin2018deepsphere, khasanova2017graphomni, boomsma2017sphericalcnn, su2017sphericalcnn, coors2018sphericalcnn,
% kondor2018sphericalcnn, => quite specific, same applications as Cohen
jiang2019sphericalcnn}.

% why graphs for the sphere
Spherical data can be seen as a continuous function
%$f: S^2 \rightarrow \R^d$
that is sampled at discrete locations.
%is sampled at convenient points, given by a chosen pixelization.
As it is impossible to construct a regular discretization of the sphere, there is no perfect spherical sampling.
Schemes have been engineered for particular applications and come with trade-offs \citep{gorski2005healpix,glesp}.
%emphasizing one or more features at the cost of others.
% For example, the equirectangular (a.k.a.\ equiangular or geographic) grid, used in most spherical CNNs, is most similar to a 2D grid.
% HEALPix features equal-area and hierarchically organized pixels, and has a fast spherical harmonic transform (SHT) \citep{healpix}.
% GLESP features a fast and exact
% %(to machine precision)
% SHT \citep{glesp}.
% sympix?, cubed-sphere? => no space
%\todo{maybe have a table, or a list of desired properties from a sampling} => no space
However, while sampling locations can be precisely controlled in some cases (like the CMOS sensors of an omni-directional camera), sensors might in general be non-uniformly distributed, cover only part of the sphere, and move (see \figref{examples}).
Modeling the sampled sphere as a discrete graph has the potential to faithfully and efficiently represent sampled spherical data by placing vertices where data has been measured: no need to handle missing data or to interpolate to some predefined sampling, and no waste of memory or precision due to over- or under-sampling.
Graph-based spherical CNNs have been proposed in \citet{khasanova2017graphomni} and \citet{perraudin2018deepsphere}.
Moreover, graph convolutions have a lower computational cost of $\bO(N_{pix})$ compared to $\bO(N_{pix}^{2/3})$ for the SHT-based convolutions of \citet{cohen2018sphericalcnn} and \citet{esteves2017sphericalcnn}, where $N_{pix}$ is the number of considered pixels.

%\todo{Most of our views are shaped by cosmology. What is going on in other fields (climate, weather)? Do they have their own samplings? Cosmo seems however the most advanced for SP on the sphere.}

%As the sphere $\mathcal{S}^2$ is a homogeneous space of the 3D rotation group $SO(3)$.

% Finally, to exploit the rotation symmetry of certain tasks involving spherical data, we would like a rotation equivariant formulation \citep{cohen2016equivariance}.
% speak about symmetries in the conclusion

Finally, like classical 2D CNNs are equivariant to translations, we want spherical CNNs to be equivariant to 3D rotations \citep{cohen2016equivariance, kondor2018equivariance}.
% we'll nuance this view in the conclusion
A rotation-equivariant CNN detects patterns regardless of how they are rotated on the sphere: they exploit the rotational symmetries of the data through weight sharing.
Realizing that, spheres can be used to support data which does not intrinsically live on a sphere but have rotational symmetries \citep[for 3D objects and molecules]{cohen2018sphericalcnn, esteves2017sphericalcnn}.
In this contribution we present DeepSphere~\cite{perraudin2018deepsphere}, a spherical neural network leaveraging graph convolution for its speed and flexibility. Furthermore, we discuss under which setting graph convolution on the sphere can is close to equivariant to rotations.

% \todo{From \citet{kondor2018equivariance}:
% A convolution implies the equivariance to the action of some group.
% Contrary as well: equivariance requires a convolution structure.
% }

% \begin{figure}
% 	% three figures: healpix, glesp, graph
% 	\label{fig:samplings}
% \end{figure}

\section{Method}

% \todo{Basics of GSP, how the graph is built, polynomial filters.}
Our method relies on the graph signal processing framework \cite{shuman2013gsp}. In this setting  


DeepSphere, presented in \figref{architecture}, leverages convolutions on graphs and hierarchical pooling to achieve the following properties: (i) computational efficiency(ii) adaptation to irregular sampling, and (iii) close to rotation equivariance,.
The main idea is to model the discretised sphere as a graph of connected pixels: the length of the shortest path between two pixels is an approximation of the geodesic distance between them.
We use the graph CNN formulation introduced in \citep{defferrard2016convolutional}, and a pooling strategy that exploits a hierarchical pixelisation of the sphere to analyse the data at multiple scales.
The current implementation of DeepSphere relies on the Equal Area isoLatitude Pixelisation (HEALPix)~\citep{gorski2005healpix}, a popular sampling used in cosmology and astrophysics.
DeepSphere is, however, easily used with other samplings as only two elements depend on it: (i) the choice of neighboring pixels when building the graph, and (ii) the choice of parent pixels when building the hierarchy.
The flexibility of modeling the data domain with a graph allows one to easily model data that spans only a part of the sphere, or data that is not uniformly sampled. 
Furthermore, using a $k$-nearest neighbours graph, the convolution operation costs $\mathcal{O}(N_{pix})$ operations, where $N_{pix}$ is the number of pixels.
This is the lowest possible complexity for a convolution without approximations.

Neverthless, while graphs offer great flexility, their ability to represent the underlying manifold (here the sphere) and hence for their convolution to be close to rotational equivariant will, as we shall see in the next section,  highly depends on sampling choice and on the graph construction.

% DeepSphere is readily apt to solve four tasks: (i) global classification (i.e., predict a class from a map), (ii) global regression (i.e., predict a set of parameters from a map), (iii) dense classification (i.e., predict a class for each pixel of a map), and (iv) dense regression, (i.e., predict a set of maps from a map).
% Input data are spherical maps with a single value per pixel, such as the CMB temperature, or multiple values per pixel, such as surveys at multiple radio frequencies.

% While there exists many graph NNs (see for example xxx and xxx for recent reviews),
% \todo{Why this graph NN? We want an equivalence with the continuous world to prove equivariance to arbitrary rotations.}

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{figure_architecture_v3}
	\caption{Overall NN architecture, showing here three convolutional layers acting as feature extractors followed by three fully connected layers with softmax acting as the classifier.
    A convolutional layer is based on five operations: convolution, non-linearity, batch normalization, down-sampling, and pooling.
	While most operations are agnostic to the data domain, the convolution and the down-sampling have to be adapted.
	In this paper, we propose first to model the sphere with a graph and to perform the convolution on the graph.
	Graphs are versatile data structures which can model any sampling, even irregular or partial.
	Second, we propose to exploit a hierarchical pixelization of the sphere for the down-sampling operation.
	It allows the NN to analyze the data at multiple scales while preserving the spatial localization of features.
	This figure shows a network that operates on the whole sphere.
	The process is the same when working with partial observations, except that the graph is only built for the region of interest.}
	\label{fig:architecture}
\end{figure}

% \todo{We can take here a different stance than: "we'll use whatever method gives the best performance". SHTs are the standard analysis tool on the sphere. People not only want to apply learned filters, but designed ones (e.g., Gaussian smoothing), people want to interpret and see the spectrum as spectral analysis is important (at least in cosmo, we should find some refs for that}





% That should be short: it's an ML audience. We can reference previous work. No general ML stuff, as in cosmo paper.

%\section{Spherical harmonics and equivariance}
\section{Harmonics and equivariance}

% \todo{why equivalence to continuous provides equivariance. Clean way of doing it. Alternative: Renata's mechanical approach.}
often easier to describe in Fourier space \citep{kondor2018equivariance} => motivation for the spectral interpretation?

% \todo{there is a compromise about the sampling quality (how far from uniform) and how close discrete computations can be made to the true continuous ones}

\paragraph{Why is the harmonic analysis of the graph important for equivariance?}
As both the graph and the spherical convolution can be expressed as a multiplication in the Fourier domain, the spectral analysis of the graph Laplacian provides valuable insights on the equivariance property of the graph convolution.
The rationale being that as the spectral bases align, the two operations become more similar, with the consequence that the equivariance property of the spherical convolution being carried out to the graph convolution.

\paragraph{Example.}
In order to better understand this process, let us study the circle manifold $\mathcal{C}^1$ as depicted in \figref{ring}. We parametrize each point $(\cos\theta,\sin\theta)$ of $\mathcal{C}^1$ with the variable $\theta\in[0,2\pi[$.
For this manifold, the eigenfunctions of the Laplace-Beltrami operator can be expressed as $u_\ell(\theta)=e^{i \theta m \ell}$, for $\ell \in \mathbb{N}$ and $m\in\{-1,1\}$.
Interestingly, if one selects a \emph{regular sampling} on the $\mathcal{C}^1$, as shown in \figref{ring}, the sampled eigenfunctions turn out to be the discrete Fourier basis, meaning that the harmonic decomposition of a discretized function on the circle can be done using the well-known Discrete Fourier Transform (DFT).
In this case, it turns out that the DFT is also an eigenbase of the of $2$ nearest neighbors graph Laplacian \cite{strang1999discrete}. Hence, in the $\mathcal{C}^1$ case, it can be verified, under mild assumptions, that the graph convolution is equivariant to translation\footnote{See \cite[section 2.2 and equation 3]{perraudin2017stationary} for details.}
It should be noted that if an irregular sampling is selected, then the graph Fourier basis will not align the sampled manifold eigenvectors.

The argument for the graph Laplacian to be diabonizable with the DFT basis is simple: a) the graph Laplacian is symmetric circulant and that all circulant matrices have complex exponentials as eigenbases.
More generally, higher dimension circles, such as the torus $\mathcal{C}^2$, have also a circulant weight matrix and hence their associated graph is also equipped with an equivariant convolution.

\paragraph{Necessary conditions for the rotational equivariance on the sphere}
We note two important points for the equivariance property to be carried on the graph convolution: a) the sampling needs to be regular, and b), the constructed graph has to be constructed circulant,\footnote{A circulant graph is an undirected graph acted on by a cyclic group of symmetries which takes any vertex to any other vertex.} i.e. it has a circulant weight matrix.
Unfortunately, there exist no regular sampling for a sphere $\mathcal{S}^2$ of more than $12$ points. Hence it is in general not possible to build a graph with a convolution that is equivariant to rotation\footnote{The exception is the dense graph. Unfortunately, in the extreme case the graph convolution is limited to Kronecker kernels, which makes it useless.}.



% \subsection{Boundary conditions}

% The graph setting used throughout this contribution corresponds to assumed reflective border conditions.
% While that is irrelevant when working on the complete sphere (as it has no border), it slightly affects the convolution operation when only a part of the sphere is considered.
% As depicted in \figref{border_effects}, a filter localized near a border (via $h(\L) \b \delta_i$) is no longer isotropic.
% These border effects can, however, be mitigated by padding with zeros a small area around the part of interest (in which case they become similar to border effects in classical CNNs).
% We however do not expect these effects to cause any problem as long as the data samples cover the same area.

% \begin{figure}[ht!]
% 	\centering
% 	\includegraphics[width=\linewidth]{border_effects}
% 	\caption{Convolution kernel (also called filter) localized in the center and left corner of a graph built from $1/12^\text{th}$ of the sphere at $N_{side} = 16$.
% 		A filter $h$ is localized on a pixel $i$ as $\T_i h = h(\L) \b \delta_i$ (see \secref{graph_convolution}, equation~\eqnref{graph_convolution_spatial}).
% 	The filter is not isotropic anymore when localized on the corner as the graph representation of a manifold assumes reflective border conditions.}
% 	\label{fig:border_effects}
% \end{figure}


% Proper study of symmetries and equivariance requires a continuous treatment.
% @Michael: Not necessarily. It is more complicated than that. For example, if you work in the spectral domain, you can bypass sampling problem. This is what happen with the work of Cohen. So I think we should not speak about that.
% For example, only rotations by $90^\circ$ can be studied on a discrete sampling grid.


% \subsection{Convergence proof for random unform sampling}

% Let us use the angular parametrization of the sphere $\mathcal{S}^2=\{\bf{x} | \|\bf{x}\|_2=1 \}$, in term of angles of zenith, $\theta\in [0,\pi ]$, and azimuth $\phi\in [0,2\pi[$.
% \begin{equation}
% \Phi(\theta, \phi) = (\sin \theta \cos \phi, \sin \theta \sin \phi, \cos \theta )
% \end{equation}
% Let $\mathcal(C)^\infty(\mathcal{S}^2)$ denote the class of infinitely differentiable functions $\mathcal{S}^2\rightarrow \mathbb{R}$. Using the agnular parametrization of the sphere, the Laplace-Beltrami for a function $f\in \mathcal(C)^\infty(\mathcal{S}^2)$ is given by:
% \begin{equation}
% \Delta_{\mathcal{S}^2} f = \frac{1}{\sin \theta } \frac{\partial}{\partial \theta} \left(\sin \theta  \frac{\partial f}{\partial \theta} + \frac{1}{\sin^2 \theta} \frac{\partial^2f}{\partial \phi^2} \right)
% \end{equation}

% Let us extend the graph Laplacian

% \begin{equation}
% \L \b{f}[i] := \frac{1}{n} \left( \b{f}[i]\sum_{j=1}^n k_t(x_i,x_j) -\sum_{j=1}^n \b{f}[j] k_t(x_i,x_j)\right),
% \end{equation}
% where $k_t(x_i,x_j)=e^{-\frac{x_i-x_j}{4t}}$, \label{eq:full_graph_gaussian_weight}
% to each point on the Sphere.

%  It is a generalization of the graph Laplacian operator that apply on a function $f:\mathcal{S}^2 \rightarrow \mathbb{R} $:
% \begin{equation}
% L_t f(y) := \frac{1}{n} \left( f(y)\sum_{j=1}^n k_t(y,x_j)-\sum_{j=1}^n f(x_j)k_t(y,x_j) \right)
% \end{equation}


%  Given a function $f\in \mathcal{C}^\infty(\mathcal{S}^2)$ and for a fixed point $p \in \mathcal{C}^2$, after appropriate scaling the operator $L_t$, converge toward of the  Laplace–Beltrami operator.

% \begin{theorem}[Adaptation of Theorem 3.1 of~\cite{belkin2005towards}, Laplacian convergence] \label{theo:laplacian_convergence_belkin}
% Let data points $\b{x}_1,\dots, \b{x}_N$ be sampled from a uniform distribution on a manifold $\mathcal{M} \subset \mathbb{R}^T$.
% Using the weight function \eqref{eq:full_graph_gaussian_weight}, define a sequence $t_N=N^{-\frac{1}{4+\alpha}}$, where $\alpha>0$ and let $f\in \mathcal{C}^\infty(\mathcal{C}^2)$, then the following holds:
% \begin{equation}
% \lim_{N \rightarrow \infty} \frac{1}{t_N(4\pi t_N)} (L \b{f})[i] = \frac{1}{\text{vol} (\mathcal{C}^2)} (\Delta_{\mathcal{C}^2} f)(\b{x}_i)
% \end{equation}
% where the limit is taken in probability and $\text{vol}(\mathcal{C}^2)=4 \pi$ is the surface of the sphere with respect to the canonical measure.
% \end{theorem}

% \subsection{Empirical analysis of the graph eigenvectors}

% The first 16 eigenvectors $[\b u_1, \ldots, \b u_{16}]$ of the graph Laplacian $\L$, forming the lower part of the graph Fourier basis $\U$, are shown in \figref{graph_harmonics}.

\paragraph{Empirical analysis of DeepSphere graph Laplacian:}
% Let us further observe the spectral properties of our constructed spherical graph laplacian $\L$.
Given the graph used for DeepSphere, let us analyse the spectral alignment of the graph Fourier basis and the spherical harmonics.
The graph Laplacian eigenvalues, shown in \figref{graph_eigenvalues}, are clearly organized in frequency groups of $2\ell + 1$ orders for each degree $\ell$.
% We remind the reader that the amplitude of the Laplacian eigenvalue is proportional to the sum of the variations of its associated eigenvector.
% Furthermore, all spherical harmonics with the same order $\ell$ have the same variation.
% Hence, the fact that the Laplacian eigenvalues are grouped in blocks of size $2\ell + 1$ is a hint that the graph eigenvectors approximate the spherical harmonics.
These blocks corresponds to the different eigenspaces of the spherical harmonics.
We also show the correspondence between the subspaces spanned by the graph Fourier modes and the spherical harmonics in \figref{subspace_harmonics_eigenvectors}.
%The block-diagonal structure of the matrix, as opposed to diagonal, results the fact that the phase of the graph eigenvector is not aligned with the one of the spherical harmonics.
For $N_{side}=4$, we clearly observe a good allignment for $\ell\leq 8$. This mean that for kernel with low frequencies, the graph convolution will be rotational equivariant. On the contrary, a kernel with higher frequencies ($\ell>8$) will not be.

% While these indications show that the constructed graph Fourier basis approximates well the spherical harmonics,
% one should not forget that
The imperfection in the alignment is likely due to the small irregularities in the HEALPix sampling (non-constant number of neighboring pixels and varying distance between pixels, \todo{see \figref{healpix_graph_4})} have an important effect on the graph Fourier modes.
% First, we believe that they are responsible for energy leaking across frequency bands in \figref{subspace_harmonics_eigenvectors}.
Furthermore, the graph construction scheme might not be optimal as the resulting Laplacian is probably far from circulant. One construction scheme that might be more adapted is a fully connected graph as this scheme is used in convergence proof in \cite{belkin2005towards,belkin2007convergence}.
% Second, as the resolution is increased with $N_{side} \rightarrow \infty$ and $N_{pix} \rightarrow \infty$, we are still unsure if the eigenvectors would converge towards the spherical harmonics.
% We believe that it might be the case for a different construction of the graph (fully connected) and that a proof could be built on top of the work of~\cite{belkin2007convergence}. That is, however, out of the scope of this contribution.
% The theoretical study of those phenomenons is left as future work.
% Third, counter-intuitively, some eigenvectors will be localized~\citep{perraudin2018global}, i.e., they will span a small part of the sphere.
% Those discrepancies result in a convolution operation that is not exactly equivariant to rotation.
% Nevertheless, our experiments suggest that these downsides do not have an important effect on the convolution nor hinder the performance of the NN.

\begin{figure}[ht!]
\centering
\begin{minipage}{.2\textwidth}
	\centering
	\includegraphics[width=\linewidth]{ring}
	\captionof{figure}{Green: circle manifold $\mathcal{C}^1$. Red: the regular sampled points and graph vertices. Blue: graph edges.}
	\label{fig:ring}
\end{minipage}
\hspace{1cm}
\begin{minipage}{.7\textwidth}
	\centering
	\includegraphics[width=\linewidth]{graph_eigenvalues}
	\captionof{figure}{The eigenvalues $\bLambda$ of the graph Laplacian $\L = \U \bLambda \U\trans$, which corresponds to squared frequencies, are clearly organized in groups. Each group corresponds to a degree $\ell$ of the spherical harmonics. Each degree has $2\ell + 1$ orders.
	See also \figref{graph_harmonics}.
	}
	\label{fig:graph_eigenvalues}
\end{minipage}
\end{figure}

\begin{figure}[ht!]
	\centering
	\includegraphics[width=0.9\linewidth]{subspace_harmonics_eigenvectors_v2}
	\caption{Correspondence between the subspaces spanned by the graph Fourier modes and the spherical harmonics.
		The correspondance is computed in two steps. (i) We compute the PSD of each graph eigenvector.
		(ii) As, for each harmonic index $\ell$, there is $2\ell+1$ spherical harmonics, we sum the contribution of the corresponding $2\ell+1$ graph eigenvectors.
		% No real convergence with increasing Nside.
		% Row $j$ is the sum of the (normalized) power spectral densities (PSDs), computed by the SHT, of the $2j+1$ graph Fourier modes $\b u_i$ identified with degree $j$.
		% (\figref{graph_eigenvalues} shows how Fourier modes can be identified to spherical degrees.)
		% The PSD is computed for degrees $\ell=0$ to $\ell = 3 N_{side} -1$.
		% Pixel $k$ in row $j$ measures the energy in frequency band $k$ of the graph Fourier modes identified to be part of band $j$.
		The matrix should ideally be the identity.
		While not the case, the plot shows how the subspaces agree: the subspaces spanned by the Fourier modes correspond to those spanned by the spherical harmonics in the low frequencies, while the Fourier modes leak towards adjacent frequency bands in the high frequencies.
		While there is a systematic error, the Fourier modes agree at higher frequencies as $N_{side}$ increases.}
		\label{fig:subspace_harmonics_eigenvectors}
\end{figure}



\todo{new results from Martino: becomes better in BN setting, currently under study}

\todo{we have less rich operations (compared to the most general linear equivariant map) by restricting our filters to be radial, but does it matter in practice?}
\todo{isotropic filters provide invariance to the third rotation}

\section{Experiments}

\todo{the cosmo experiment}

\todo{maybe mention some preliminary results on SHREC-17: we could say performance are so far mostly similar (while being invariant to the third rotation), still under study}

% \begin{figure}
% 	\includegraphics[height=0.2\linewidth]{example_model_esteves}
% 	\includegraphics[height=0.2\linewidth]{example_model_projected_esteves}
% 	\caption{
% 		Projection of a 3D model on the sphere.
% 		Typical task on which spherical CNNs are evaluated.
% 		Compared to the examples in \figref{spherical_data_examples}, the data covers the whole sphere and is regularly sampled.}
% \end{figure}

\section{Conclusion}  % & perspective

\todo{Generalization vs specialization: most general is to assume no symmetries (fully connected NN).
NNs can be specialized by adding some equivariance and invariance to symmetry groups such as translation, rotation, flip, etc.
More specialized NNs are more limited in the class of functions they can approximate, but requires less samples.
Again, use the right symmetries.}

exploit symmetries in the data by being equivariant (or invariant) to a select group of symmetric transformations (symmetry group).
In this paper, we argue that graph neural networks are a flexible model (albeit not the most general) to exploit such symmetries.

\todo{As equivariance is not the Graal either. We expect the desired amount of equivariance to depend on the data and task. As usual, practitioners should adopt a NN architecture that exploit the symmetries of their problem. \citep{coors2018sphericalcnn, jiang2019sphericalcnn}.}

\todo{future directions: equivalence / convergence to spherical harmonics, other tasks, comparison with other spherical CNNs, graph CNNs (especially those for manifolds), boundary conditions on partial sphere}

\textbf{Long term vision.}

We hope to establish graphs as a generic support for processing and learning over known and unknown manifolds.
\todo{Applications are plenty:
known manifolds (1D, 2D, 3D Euclidean spaces, circle, sphere)
% Minkowski spacetime
unknown manifolds (shapes, point clouds, feature sets)
}
\todo{
* known manifolds: grid, sphere, spacetime (a pseudo-Riemannian 4-manifold), surfaces defined by NURBS, anything else?
* unknown manifolds: meshes (human body), point clouds => what are the symmetries? local isometry
* non-manifold: all networks (brain, social, transportation, telecom, etc.) and relations (author-papers, user-products)
}

\todo{Similar goal: graph CNNs for group equivariant convolution.}

The vision is to push the use of graphs as the support for computation on manifolds. The advantages of graphs are to relax constraints on the sampling (e.g., allow an irregular or partial sampling), and be computationally more efficient (while being exact w.r.t. the continuous case).
Part of a broader research effort to explore the use of GSP to solve continuous problems.

\subsubsection*{Acknowledgments}

\todo{Pierre, Cardoso, Tomek?}

\bibliography{refs}
\bibliographystyle{iclr2019}

\end{document}
